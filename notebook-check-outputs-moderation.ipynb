{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f52420",
   "metadata": {},
   "source": [
    "# Moderation Checks\n",
    "1. without ideal answers, you can write a rubric, use one LLM to evaluate another LLM's output\n",
    "2. with expert provided ideal answers, LLM can compare if a specific assisstant output is similar to expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60055e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import utils\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "print (openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ceaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0.6, max_tokens=3000):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    ### token_dict = {'prompt_tokens': response['usage']['prompt_tokens']}\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response_to_custoemer = f\"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# moderations check\n",
    "moderation_output = openai.Moderation.create(\n",
    "    input=final_response_to_custoemer)[\"result\"][0]\n",
    "print(moderation_output) # will output a flag and metrics regarding hate/violence/sexsual/self-harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7f1b98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# moderations check\u001b[39;00m\n\u001b[1;32m      2\u001b[0m moderation_response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mModeration\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mresponse\u001b[49m\n\u001b[1;32m      4\u001b[0m )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(moderation_response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7051ba",
   "metadata": {},
   "source": [
    "# Evaluations / Audit LLM with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40e86f",
   "metadata": {},
   "source": [
    "## using chain of thought reasoning prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce82c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audit responses regulations\n",
    "system_message = f\"\"\"\n",
    "You are an assisstant that evaluates whether \\\n",
    "customer service agent responses suffciently \\\n",
    "answer customer questions, and also validates that \\\n",
    "all the facts the assistant cites from the product \\\n",
    "information are correct.\n",
    "The product information and user and customer \\\n",
    "service agent messages will be dilimted by \\\n",
    "3 backticks, i.e. ```.\n",
    "\n",
    "Respond with a Y or N character, with no punctuation:\n",
    "Y - if the output sufficiently answers the question \\\n",
    "AND the response correctly users product information\n",
    "N - otherwise\n",
    "\n",
    "Output a single letter only.\n",
    "\"\"\"\n",
    "\n",
    "customer_message = f\"\"\"\n",
    "tell me about the smartx pro phone\"\"\"\n",
    "\n",
    "final_response_to_customer = f\"\"\"\n",
    "The SmartX ProPhone has a 6.1 inch display, 100W output, \\\n",
    "wireless subwoofer and Bluetooth.\\\n",
    "Do you have any specific questions \\\n",
    "about these products or any other products we offer?\n",
    "\"\"\"\n",
    "\n",
    "product_information = \"\"\"{'name':'SmartX ProPhone', 'category':'Phone'}\"\"\"\n",
    "\n",
    "q_a_pair = f\"\"\"\n",
    "Customer message: ```{customer_message}```\n",
    "Product information: ```{product_information}```\n",
    "Agent response: ```{final_response_to_customer}```\n",
    "\n",
    "Does the response use the retrieved information correctly?\n",
    "Dose the response sufficiently answer the question?\n",
    "\n",
    "Output Y or N\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_message},\n",
    "    {'role': 'user', 'content': q_a_pair}\n",
    "]\n",
    "\n",
    "response  = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f50aca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79fb77fb",
   "metadata": {},
   "source": [
    "# with rubric, but no expert provided ideal answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vs_rubric(test_set, assistant_answer):\n",
    "    cust_msg = test_set['customer_msg']\n",
    "    context = test_set['context']\n",
    "    completion = assistant_answer\n",
    "    \n",
    "    system_message = \"\"\"\\\n",
    "    You are an assitant that evaluates how well the customer service agent\n",
    "    answers a user question by looking at the context that the\n",
    "    agent is using to generate its response.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"\\\n",
    "    You are evaluating a submitted answer to a question based on a context\n",
    "    that the agent uses to answer the question.\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]:{cust_msg}\n",
    "    ************\n",
    "    [Context]:{context}\n",
    "    ************\n",
    "    [Submission]:{completion}\n",
    "    ************\n",
    "    [END DATA]\n",
    "    \n",
    "    Compare the factual content of the submitted answer with the context.\n",
    "    Ignore any differences in styele, grammar, or punctuation.\n",
    "    Answer the following questions:\n",
    "    \n",
    "    - Is the Assistant response based only on the context? (Y or N)\n",
    "    - Does the answer include information that is not provied in the context? (Y or N)\n",
    "    - Is there any disagreement between the response and the context? (Y or N)\n",
    "    - Count how many question the user asked.(output a number)\n",
    "    - For each question that user asked, is there a corresponding answer?\n",
    "      Question 1: (Y or N)\n",
    "      Question 2: (Y or N)\n",
    "      ...\n",
    "      Question N: (Y or N)\n",
    "    - Of the number of questions asked, how many of there question were addressed by Assistant?\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    response  = get_completion_from_messages(messages)\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22762c4d",
   "metadata": {},
   "source": [
    "## with expert provided ideal answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2b220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# products_by_category = utils.get_product_from_query(customer_msg)\n",
    "# category_and_product_list = utils.read_string_to_list(products_by_category)\n",
    "# product_info = utils.get_mentioned_product_info(category_and_product_list)\n",
    "# assistant_answer = utils.answer_user_msg(user_msg=customer_msg, product_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7a61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vs_ideal(test_set, assistant_answer):\n",
    "    cust_msg = test_set['customer_msg']\n",
    "    ideal = test_set['ideal_answer']\n",
    "    completion = assistant_answer\n",
    "    \n",
    "    system_message = \"\"\"\\\n",
    "    You are an assitant that evaluates how well the customer service agent\n",
    "    answers a user question by comparing the response to the ideal (expert) response.\n",
    "    Output a single letter and nothing else.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"\\\n",
    "    You are comparing a submitted answer to an expert answer on a given question\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]:{cust_msg}\n",
    "    ************\n",
    "    [Expert]:{ideal}\n",
    "    ************\n",
    "    [Submission]:{completion}\n",
    "    ************\n",
    "    [END DATA]\n",
    "    \n",
    "    Compare the factual content of the submitted answer with expert answer. Ignore any differences in style, grammar or punctuation.\n",
    "    The submitted answer may either be a subset or superset of expert answer, or it may conflict. Determine which case applies. Answer the questions:\n",
    "    (A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "    (B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "    (C) The submitted answer contains all the same details to the expert answer.\n",
    "    (D) There is a disagreement between the submitted answer and the expert answer.\n",
    "    (E) The answers differ, but these differences don't matter from perspectives of factual.\n",
    "    choice_strings: ABCDE\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    response  = get_completion_from_messages(messages)\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e9016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
